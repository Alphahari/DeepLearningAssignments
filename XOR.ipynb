{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped due to lack of improvement in epoch 9\n",
      "Training stopped due to lack of improvement in epoch 9\n",
      "Training stopped due to small weight changes in epoch 0\n",
      "XOR(0, 0)=1\n",
      "Training stopped due to lack of improvement in epoch 9\n",
      "Training stopped due to small weight changes in epoch 5\n",
      "Training stopped due to lack of improvement in epoch 9\n",
      "XOR(0, 1)=0\n",
      "Training stopped due to lack of improvement in epoch 9\n",
      "Training stopped due to small weight changes in epoch 5\n",
      "Training stopped due to lack of improvement in epoch 9\n",
      "XOR(1, 0)=0\n",
      "Training stopped due to lack of improvement in epoch 9\n",
      "Training stopped due to lack of improvement in epoch 9\n",
      "Training stopped due to lack of improvement in epoch 9\n",
      "XOR(1, 1)=1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the unit step function\n",
    "# This function returns 1 if input v >= 0, else it returns 0.\n",
    "def unitStep(v):\n",
    "    if v >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Perceptron model that computes output based on inputs (x), weights (w), and bias (b)\n",
    "# It calculates the dot product of weights and input, adds bias, then applies the unit step function.\n",
    "def perceptronModel(x, w, b):\n",
    "    v = np.dot(w, x) + b  # Linear combination of weights and input\n",
    "    y = unitStep(v)  # Apply step function to get binary output\n",
    "    return y\n",
    "\n",
    "# Training function for perceptron with dynamic stopping\n",
    "# This function updates weights and bias until either weights stop changing or no classification error is made.\n",
    "def train_perceptron_dynamic(X, y, w, b, learning_rate=0.01, max_epochs=1000, tolerance=1e-4, patience=10):\n",
    "    weights = w  # Initialize weights\n",
    "    bias = b  # Initialize bias\n",
    "    prev_weights = np.copy(weights)  # Keep track of previous weights for comparison\n",
    "    patience_counter = 0  # To implement early stopping if no improvement\n",
    "\n",
    "    for epoch in range(max_epochs):  # Loop over each epoch (training iteration)\n",
    "        total_error = 0  # Reset total error for each epoch\n",
    "        for i in range(len(X)):  # Loop through each input data point\n",
    "            y_pred = perceptronModel(X[i], weights, bias)  # Predict the output\n",
    "            error = y[i] - y_pred  # Calculate the error (difference between actual and predicted output)\n",
    "            total_error += abs(error)  # Accumulate total error\n",
    "            # Update weights and bias if there's an error\n",
    "            weights += learning_rate * error * X[i]\n",
    "            bias += learning_rate * error\n",
    "\n",
    "        # Check weight change from previous epoch\n",
    "        weight_change = np.sum(np.abs(weights - prev_weights))\n",
    "        if weight_change < tolerance:  # If the change is less than the tolerance, stop training\n",
    "            print(f\"Training stopped due to small weight changes in epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        # Update previous weights for next comparison\n",
    "        prev_weights = np.copy(weights)\n",
    "\n",
    "        # Early stopping based on error improvement\n",
    "        if total_error == 0:  # If total error is 0, all predictions are correct, so stop training\n",
    "            print(f\"Training stopped due to zero classification error in epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        # Patience mechanism: stop if no improvement over `patience` epochs\n",
    "        if total_error > 0:  # If there's still error, increment the patience counter\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:  # If patience limit is reached, stop training\n",
    "                print(f\"Training stopped due to lack of improvement in epoch {epoch}\")\n",
    "                break\n",
    "        else:\n",
    "            patience_counter = 0  # Reset patience counter if there was improvement\n",
    "\n",
    "    return weights, bias  # Return the updated weights and bias after training\n",
    "\n",
    "# NOT logic function using a perceptron\n",
    "# This function uses predefined weights and bias for the NOT gate.\n",
    "def NOT_logicFunction(x, wNOT, bNOT):\n",
    "    return perceptronModel(x, wNOT, bNOT)\n",
    "\n",
    "# AND logic function using a perceptron\n",
    "# This function applies weights and bias specific to the AND logic gate.\n",
    "def AND_logicFunction(x, w1, bAND):\n",
    "    return perceptronModel(x, w1, bAND)\n",
    "\n",
    "# OR logic function using a perceptron\n",
    "# Weights and bias specific to OR gate are used here.\n",
    "def OR_logicFunction(x, w2, bOR):\n",
    "    return perceptronModel(x, w2, bOR)\n",
    "\n",
    "# XOR logic function using a combination of AND, OR, and NOT gates\n",
    "# This combines basic gates to simulate the XOR function, which isn't linearly separable.\n",
    "def XOR_logicFunction(x):\n",
    "    # Initialize weights and biases for NOT, AND, and OR gates using random values\n",
    "    wNOT = (np.random.rand())  # Random initialization for NOT gate weight\n",
    "    bNOT = (np.random.rand())  # Random initialization for NOT gate bias\n",
    "    x1 = np.array([0, 1])  # Input for NOT gate\n",
    "    y1 = np.array([1, 0])  # Expected output for NOT gate\n",
    "    wNOT, bNOT = train_perceptron_dynamic(x1, y1, wNOT, bNOT)  # Train NOT gate perceptron\n",
    "\n",
    "    # Random initialization for AND gate weights and bias\n",
    "    w1 = np.random.rand(1, 2)\n",
    "    bAND = -(np.random.rand())\n",
    "    y2 = np.array([0, 0, 0, 1])  # Expected output for AND gate\n",
    "    w1, bAND = train_perceptron_dynamic(X, y2, w1, bAND)  # Train AND gate perceptron\n",
    "\n",
    "    # Random initialization for OR gate weights and bias\n",
    "    w2 = np.random.rand(1, 2)\n",
    "    bOR = -(np.random.rand())\n",
    "    y3 = np.array([0, 1, 1, 1])  # Expected output for OR gate\n",
    "    w2, bOR = train_perceptron_dynamic(X, y3, w2, bOR)  # Train OR gate perceptron\n",
    "\n",
    "    # Apply the trained AND, OR, and NOT gates to simulate XOR\n",
    "    y11 = AND_logicFunction(x, w1, bAND)  # AND(x1, x2)\n",
    "    y12 = OR_logicFunction(x, w2, bOR)  # OR(x1, x2)\n",
    "    y13 = NOT_logicFunction(y11, wNOT, bNOT)  # NOT(AND(x1, x2))\n",
    "    \n",
    "    final_x = np.array([y12, y13])  # Combine outputs of OR and NOT-AND\n",
    "    finalOutput = AND_logicFunction(final_x, w1, bAND)  # Apply AND to simulate XOR behavior\n",
    "    return finalOutput\n",
    "\n",
    "# Input and expected output for XOR function testing\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input combinations for XOR\n",
    "Y = np.array([0, 1, 1, 0])\n",
    "\n",
    "\n",
    "print(\"XOR({}, {})={}\".format(X[0][0], X[0][1], XOR_logicFunction(X[0])))\n",
    "print(\"XOR({}, {})={}\".format(X[1][0], X[1][1], XOR_logicFunction(X[1])))\n",
    "print(\"XOR({}, {})={}\".format(X[2][0], X[2][1], XOR_logicFunction(X[2])))\n",
    "print(\"XOR({}, {})={}\".format(X[3][0], X[3][1], XOR_logicFunction(X[3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "XOR(0.0, 0.0) = 0.0272\n",
      "XOR(0.0, 1.0) = 0.9890\n",
      "XOR(1.0, 0.0) = 0.9890\n",
      "XOR(1.0, 1.0) = 0.0079\n",
      "XOR(0.0, 0.0) = 0\n",
      "XOR(0.0, 1.0) = 1\n",
      "XOR(1.0, 0.0) = 1\n",
      "XOR(1.0, 1.0) = 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the XOR input and output\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]], dtype=np.float32)\n",
    "\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(40, activation='relu', input_shape=(2,)),  # Hidden layer with 2 neurons\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10000, verbose=0)\n",
    "\n",
    "# Test the model\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print results\n",
    "for i, test in enumerate(X):\n",
    "    print(f\"XOR({test[0]}, {test[1]}) = {predictions[i][0]:.4f}\")\n",
    "\n",
    "for i, test in enumerate(X):\n",
    "    print(f\"XOR({test[0]}, {test[1]}) = {round(predictions[i][0])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
